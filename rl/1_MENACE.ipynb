{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HSKA AI-Lab: Reinforcement Learning - Übung 01\n",
    "\n",
    "<br>\n",
    "\n",
    "## Mount Google Drive\n",
    "\n",
    "(Bei Ausführung innerhalb von Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/My\\ Drive/ai-lab/rl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "\n",
    "## MENACE\n",
    "\n",
    "In diesem Notebook geht es darum einen ersten praktischen Einblick in die Implemtentierung eines simplen RL-Agenten zu erhalten.\n",
    "Daher soll im folgenden eine einfache Implementierung des bereits in den Folien vorgestellten [MENACE](https://en.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine), ein RL Agent zur Lösung von Tic-Tac-Toe, umgesetzt werden. Dies erfolgt mit Hilfe des RL Frameworks [OpenAI Gym](https://gym.openai.com/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install gym[atari]==0.21.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tic-Tac-Toe Umgebung\n",
    "\n",
    "Als ersten Schritt wollen wir die Umgebung (Environment) für MENACE modellieren, sprich das Tic-Tac-Toe Spielfeld.\n",
    "Dies besteht bekanntermaßen aus einer $3 x 3$ Matrix, welche wie folgt durchnummeriert wird:\n",
    "<br>\n",
    "\n",
    "```\n",
    "| 0 | 1 | 2 |\n",
    "-------------\n",
    "| 3 | 4 | 5 |\n",
    "-------------\n",
    "| 6 | 7 | 8 |\n",
    "-------------\n",
    "```\n",
    "Der Zustand des Spiels kann also jederzeit durch ein neun-elementiges Feld mit String-Werten beschrieben werden, wobei jedes Element entweder mit `' '` für ein leeres Feld oder mit `'X'` bzw. `'O'` für gespielte Felder des jeweiligen Spielers belegt ist. Beipielsweise repräsentiert `[' ', 'X', 'O', ' ', 'X', ' ', ' ', ' ' , ' ']` die folgende Spielsituation:\n",
    "\n",
    "```\n",
    "|   | X | O |\n",
    "-------------\n",
    "|   | X |   |\n",
    "-------------\n",
    "|   |   |   |\n",
    "-------------\n",
    "```\n",
    "\n",
    "Ein Agent bestimmt in seinem Zug immer das Feld, auf das seine Markierung als nächstes gesetzt wird, indem er den Index des entsprechenden Felds als Aktion wählt.\n",
    "Dementsprechen ergibt sich folgender Zustands- und Aktionsraum (`action_space` und `observation_space`, s.u.):\n",
    "<br>\n",
    "$A = \\{1,...,9\\} $ <br>\n",
    "$S = \\{1,...,9\\} $\n",
    "\n",
    "Diese Umgebung wollen wir nun im ersten Schritt als OpenAI Gym Environment implementieren."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aufgabe 1: Modellierung von Tic-Tac-Toe als Environment\n",
    "\n",
    "Dazu soll die Umgebung als eigene Klasse implementiert werden, die von `gym.Env` erbt.\n",
    "Die abstrakte Klasse `Env` erfordert die Implementierung der Methoden `step`, `reset` und `render`. Hierbei beinhaltet `step` die Logik eines Spielzugs, mittels `reset` kann die Umgebung nach einem Spiel wieder zurückgesetzt werden und `render` erzeugt eine menschenlesbare Repräsentation der Umgebung."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class Rewards(Enum):\n",
    "    \"\"\"Models the rewards of Tic-Tac-Toe\"\"\"\n",
    "    WINNER = 3\n",
    "    DRAW = 1\n",
    "    NO_REWARD = 0\n",
    "\n",
    "# size of the noughts-and-crosses board\n",
    "BOARD_SIZE = 9\n",
    "\n",
    "\n",
    "class NoughtsAndCrossesEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Models the environment of noughts-and-crosses\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(BOARD_SIZE)\n",
    "        self.observation_space = spaces.Discrete(BOARD_SIZE)\n",
    "        self.seed(seed=42)\n",
    "        self.board = None  # representation of the board (see above)\n",
    "        self.done = False\n",
    "        self.current_actor_symbol = \"-\" # the symbol of the currently active actor\n",
    "        self.reset()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"| 0 | 1 | 2 |  *  | {self.board[0]} | {self.board[1]} | {self.board[2]} | \\n\" \\\n",
    "               f\"------------   *  ------------| \\n\" \\\n",
    "               f\"| 3 | 4 | 5 |  *  | {self.board[3]} | {self.board[4]} | {self.board[5]} | \\n\" \\\n",
    "               f\"------------   *  ------------| \\n\" \\\n",
    "               f\"| 6 | 7 | 8 |  *  | {self.board[6]} | {self.board[7]} | {self.board[8]} | \\n\" \\\n",
    "               f\"------------   *  ------------| \\n\"\n",
    "\n",
    "    def _get_obs(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the current observation\n",
    "\n",
    "        Returns:\n",
    "             The current board situation\n",
    "        \"\"\"\n",
    "        return self.board\n",
    "\n",
    "    def _add_action(self, action: int, actor_symbol: str) -> None:\n",
    "        \"\"\"\n",
    "        Adds the given action to the bord, i.e., places the symbol of the actor at the index given as action.\n",
    "\n",
    "        Args:\n",
    "            action: action of the player (index where to put the corresponding symbol)\n",
    "            actor_symbol: symbol of the actor which is added at the given board position (action)\n",
    "\n",
    "        \"\"\"\n",
    "        self.board[action] = actor_symbol\n",
    "\n",
    "    def _has_winner(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks whether the current board situation is a win.\n",
    "\n",
    "        Returns:\n",
    "            True, if player with last move has won\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "\n",
    "    def _is_draw(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether the current board situation corresponds to a draw.\n",
    "\n",
    "        Returns:\n",
    "            True, if the board signals a draw\n",
    "        \"\"\"\n",
    "        return all(field != ' ' for field in self.board)\n",
    "\n",
    "    def _available_fields(self) -> List[int]:\n",
    "        \"\"\"\n",
    "        Get the indexes of the available fields\n",
    "\n",
    "        Returns:\n",
    "            List of indexes of available fields\n",
    "        \"\"\"\n",
    "\n",
    "        return [pos for pos, val in enumerate(self.board) if val == ' ']\n",
    "\n",
    "    def reset(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Resets the Environment to be ready for a new game\n",
    "\n",
    "        Returns:\n",
    "            the current observation (state of the board)\n",
    "        \"\"\"\n",
    "        self.board = [' ' for _ in range(BOARD_SIZE)]\n",
    "        self.done = False\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action: int) -> Tuple[List[str], Rewards, bool]:\n",
    "        \"\"\"\n",
    "        Do one step within the environment by applying the action.\n",
    "        To do so, the following steps are required:\n",
    "            1) Check the validity of the given action\n",
    "            2) Add the given action to the board using the current actor symbol\n",
    "            3) Check whether the last action produces a win or a draw and return observation, reward and done accordingly\n",
    "\n",
    "        Args:\n",
    "            action: Location on the board where the current player places his mark\n",
    "\n",
    "        Returns:\n",
    "            Observation - current board representation\n",
    "            Reward - reward assigned to this step\n",
    "            Done - boolean information whether the game is finished\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        if mode == \"human\":\n",
    "            return str(self)\n",
    "\n",
    "    def set_current_actor_symbol(self, actor_symbol: str) -> None:\n",
    "        \"\"\"\n",
    "        Set the given symbol as current actor symbol\n",
    "        Args:\n",
    "            actor_symbol: symbol of the current actor\n",
    "        \"\"\"\n",
    "        assert actor_symbol in ['O', 'X']\n",
    "\n",
    "        self.current_actor_symbol = actor_symbol\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Des weiteren schreiben wir uns eine kleine Hilfsfunktion, die basierend auf einem beobachteten Spielzustand die noch verfügbaren Felder zurück gibt:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def available_fields_in_state(state: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get the indexes of the available fields for the given state\n",
    "    Returns:\n",
    "         List of indexes of available fields\n",
    "    \"\"\"\n",
    "    return [pos for pos, val in enumerate(state) if val == ' ']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "\n",
    "Nachdem wir nun die Umgebung implementier haben, sollen als nächstes verschiedene Formen eines Spielers implementiert werden, u.a. MENACE.\n",
    "Zunächst definieren wir die allgemeine Struktur eines Spielers:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "class NoughtsAndCrossesActor(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class of an actor for noughts-and-crosses\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actor_symbol: str = '-'):\n",
    "        self.actor_symbol = actor_symbol\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_game(self) -> None:\n",
    "        \"\"\"\n",
    "        Preparation of the actor before the game starts.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_action(self, state: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the action of the actor for the given board constellation.\n",
    "\n",
    "        Args:\n",
    "            state: current, observed state of the game board\n",
    "\n",
    "        Returns:\n",
    "            [int] index of the board field where the actor wants to place his mark\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def win(self) -> None:\n",
    "        \"\"\"\n",
    "        Is called when the actor has won and rewards the actor accordingly.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def draw(self) -> None:\n",
    "        \"\"\"\n",
    "        Is called when the actor has played draw and rewards the actor accordingly.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def lose(self) -> None:\n",
    "        \"\"\"\n",
    "        Is called when the actor has lost the game and punishes the actor accordingly.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(f\"No stats available\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Damit später auch der trainierte MENACE-Agent zu einem Duell herausgefordert werden kann, implementieren wir einen menschlichen Spieler wie folgt:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HumanPlayer(NoughtsAndCrossesActor):\n",
    "    \"\"\"\n",
    "    Player that allows for human interactions.\n",
    "    \"\"\"\n",
    "\n",
    "    def start_game(self):\n",
    "        \"\"\"\n",
    "        Informs the player about starting match.\n",
    "        \"\"\"\n",
    "        print(\"Get ready!\")\n",
    "\n",
    "    def get_action(self, state: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Collects the player's action as console input.\n",
    "        Player needs to input the position of the field where she/he wants to place his symbol.\n",
    "        \n",
    "        Args:\n",
    "             state: the current, observed state of the board\n",
    "\n",
    "        Returns:\n",
    "            action of the human player\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            move = input('Make a move: ')\n",
    "            if int(move) in available_fields_in_state(state=state):\n",
    "                return int(move)\n",
    "            print(\"Not a valid move\")\n",
    "\n",
    "    def win(self):\n",
    "        \"\"\"\n",
    "        Informs the human player about his win.\n",
    "        \"\"\"\n",
    "        print(\"You won!\")\n",
    "\n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Informs the human player about the draw.\n",
    "        \"\"\"\n",
    "        print(\"It's a draw.\")\n",
    "\n",
    "    def lose(self):\n",
    "        \"\"\"\n",
    "        Informs the human player about his defeat.\n",
    "        \"\"\"\n",
    "        print(\"You lose.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Da wir MENACE aber nicht per Hand trainieren möchten, erstellen wir uns auch einen Agenten, der den nächsten Zug schlicht zufällig aus den noch verfügbaren Feldern wählt:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomPlayer(NoughtsAndCrossesActor):\n",
    "    \"\"\"\n",
    "    A counterpart that simply draws a random action from the available fields\n",
    "    of the present board constellation.\n",
    "    \"\"\"\n",
    "\n",
    "    def start_game(self):\n",
    "        pass\n",
    "\n",
    "    def win(self):\n",
    "        pass\n",
    "\n",
    "    def draw(self):\n",
    "        pass\n",
    "\n",
    "    def lose(self):\n",
    "        pass\n",
    "\n",
    "    def get_action(self, state: List[str]) -> int:\n",
    "        return random.choice(available_fields_in_state(state=state))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zu guter Letzt fehlt uns nun nur noch die Implementierung eines `BasicMENACE`-Agenten basierend auf `NoughtsAndCrossesActor`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aufgabe 2: Implementierung eines MENACE-Agenten"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BasicMENACE(NoughtsAndCrossesActor):\n",
    "    \"\"\"\n",
    "    Implementation of a basic MENACE actor that learns from his previous actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.matchboxes = dict()  # dictionary states: actions -> policies\n",
    "        self.wins = 0\n",
    "        self.draws = 0\n",
    "        self.defeats = 0\n",
    "        self.winning_shares = []\n",
    "        self.actions_taken: List[Tuple[str, int]] = []\n",
    "\n",
    "    def update_winning_shares(self) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the winning share after each 500 games.\n",
    "        \"\"\"\n",
    "        num_matches = [self.wins, self.draws, self.defeats]\n",
    "        if num_matches % 500 == 0:\n",
    "            self.winning_shares.append(self.wins / num_matches)\n",
    "\n",
    "    def start_game(self) -> None:\n",
    "        \"\"\"\n",
    "        Prepare MENACE for a new match and reset his taken actions.\n",
    "        They are gathered to update the mumbles in accordance to the result of the game.\n",
    "        \"\"\"\n",
    "        self.actions_taken = []\n",
    "\n",
    "    @staticmethod\n",
    "    def _initialize_matchbox(state: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Initialize a new matchbox.\n",
    "        On creation, a matchbox contains one mumble for each available field of the current board constellation.\n",
    "        There as many types of mumbles as noughts-and-cross has fields.\n",
    "        In dependence of the game progress, the amount of mumbles is multiplied to account\n",
    "        for the number of outstanding actions.  The earlier the board state the more action options are required\n",
    "        first round: 4 mumbles per available field , second round 3 mumbles...\n",
    "        The input of the method is the representation of the current board.\n",
    "        With the help of the function `available_fields_in_state`, the method should return a list of integers\n",
    "        where each integer refers to the type of one mumble.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            input:\n",
    "                state = [\"-\", \"X\", \"O\", \"-\", \"-\", \"-\", \"O\", \"X\", \"X\"]\n",
    "\n",
    "            output:\n",
    "                [0, 3, 4, 5, 0, 3, 4,5 ]\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the action MENACE decides to take given the current board constellation.\n",
    "        If MENACE is not able to take an action, it returns -1.\n",
    "        Therefore, two actions are required:\n",
    "            1) check whether the given state is already known by MENACE, if not initialize a matchbox for it\n",
    "            2) Get a random mumble of the corresponding checkbox for the given state\n",
    "\n",
    "        Returns:\n",
    "            [int] action chosen from MENACE\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def win(self) -> None:\n",
    "        \"\"\"\n",
    "        Propagates the reward of winning the game to MENACE.\n",
    "        Go through all actions taken for observed state within the current game\n",
    "        Add the corresponding mumble (amount determined by Rewards.WINNER) for the taken action to the matchbox belonging to the observed state\n",
    "        Increase winning counter and update winning shares\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def draw(self) -> None:\n",
    "        \"\"\"\n",
    "        Propagates the reward of a draw game to MENACE.\n",
    "        Go through all actions taken for observed state within the current game\n",
    "        Add the corresponding mumble (amount determined by Rewards.DRAW) for the taken action to the matchbox belonging to the observed state\n",
    "        Increase draw counter and update winning shares\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def lose(self) -> None:\n",
    "        \"\"\"\n",
    "        Propagates the reward of losing the game to MENACE.\n",
    "        \"\"\"\n",
    "        # go through all actions taken for an observed state within the current game\n",
    "        for state, marble in self.actions_taken:\n",
    "            # delete one of the corresponding marbles for the taken action\n",
    "            # from the matchbox belonging to the observed state\n",
    "            matchbox = self.matchboxes[state]\n",
    "            del matchbox[matchbox.index(marble)]\n",
    "\n",
    "        # update stats\n",
    "        self.defeats += 1\n",
    "        self.update_winning_shares()\n",
    "\n",
    "    def print_stats(self) -> None:\n",
    "        \"\"\"\n",
    "        Inform about how many board constellations MENACE knows and about the match results.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Has seen {len(self.matchboxes.keys())} board constellations\")\n",
    "        print(f\"Win/Draw/Defeat: {self.wins}/{self.draws}/{self.defeats}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nachdem wir nun bereits alle erforderlichen Komponenten implementiert haben, können wir uns nun um das Training von MENACE kümmern und uns danach einige Runden gegen ihn spielen.\n",
    "Dafür verwenden wir die folgende Implementierung eines Tic-Tac-Toe Spiels:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class NoughtsAndCrosses:\n",
    "    \"\"\"\n",
    "    Game of noughts and crosses.\n",
    "    Player one always uses 'X' as symbol and player two always 'O'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, player_one: NoughtsAndCrossesActor, player_two: NoughtsAndCrossesActor):\n",
    "        self.player_one = player_one\n",
    "        player_one.actor_symbol = 'X'\n",
    "        self.player_two = player_two\n",
    "        player_two.actor_symbol = 'O'\n",
    "        self.env = NoughtsAndCrossesEnvironment()\n",
    "\n",
    "    def play(self, silent: bool) -> None:\n",
    "        \"\"\"\n",
    "        Play one match of noughts and crosses.\n",
    "\n",
    "        :param silent: allows to deactivate the console output during the match (better for training)\n",
    "        \"\"\"\n",
    "\n",
    "        # inform both players about the starting game\n",
    "        self.player_one.start_game()\n",
    "        self.player_two.start_game()\n",
    "\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # repeat until no further action is possible\n",
    "        while not self.env.done:\n",
    "\n",
    "            # print current board constellation\n",
    "            if not silent:\n",
    "                print(self.env.render())\n",
    "\n",
    "            # get action to take from player one\n",
    "            action = self.player_one.get_action(state)\n",
    "\n",
    "            # check whether the action is invalid\n",
    "            if action == -1:\n",
    "                self.player_one.lose()\n",
    "                self.player_two.win()\n",
    "                break\n",
    "\n",
    "            self.env.set_current_actor_symbol(actor_symbol=self.player_one.actor_symbol)\n",
    "\n",
    "            state, reward, done = self.env.step(action)\n",
    "\n",
    "            # check whether the game is finished\n",
    "            if reward == Rewards.WINNER:\n",
    "                if not silent:\n",
    "                    print(self.env.render())\n",
    "                self.player_two.lose()\n",
    "                self.player_one.win()\n",
    "                break\n",
    "            if reward == Rewards.DRAW:\n",
    "                if not silent:\n",
    "                    print(self.env.render())\n",
    "                self.player_one.draw()\n",
    "                self.player_two.draw()\n",
    "                break\n",
    "\n",
    "            # print current board constellation\n",
    "            if not silent:\n",
    "                print(self.env)\n",
    "\n",
    "            # get action to take from player two\n",
    "            action = self.player_two.get_action(state)\n",
    "\n",
    "            # check whether the action is invalid\n",
    "            if action == -1:\n",
    "                self.player_two.win()\n",
    "                self.player_two.lose()\n",
    "                break\n",
    "\n",
    "            self.env.set_current_actor_symbol(actor_symbol=self.player_two.actor_symbol)\n",
    "\n",
    "            state, reward, done = self.env.step(action)\n",
    "\n",
    "            # check whether the game is finished\n",
    "            if reward == Rewards.WINNER:\n",
    "                self.player_one.lose()\n",
    "                self.player_two.win()\n",
    "                if not silent:\n",
    "                    print(self.env)\n",
    "                break\n",
    "            if reward == Rewards.DRAW:\n",
    "                self.player_one.draw()\n",
    "                self.player_two.draw()\n",
    "                if not silent:\n",
    "                    print(self.env)\n",
    "                break\n",
    "\n",
    "    def get_players(self) -> Tuple[NoughtsAndCrossesActor, NoughtsAndCrossesActor]:\n",
    "        \"\"\"\n",
    "        Get the player instances of the game\n",
    "        \"\"\"\n",
    "        return self.player_one, self.player_two\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"\n",
    "        Refreshes the environment for a new game\n",
    "        \"\"\"\n",
    "        self.env.reset()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun wollen wir damit MENACE einige Runden mittels des Zufallsspielers trainieren (spielt gerne etwas mit der Anzahl der Trainingsläufe herum)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train MENACE with a randomly playing opponent\n",
    "training_game = NoughtsAndCrosses(player_one=BasicMENACE(), player_two=RandomPlayer())\n",
    "for i in range(75000):\n",
    "    training_game.play(silent=True)\n",
    "    training_game.reset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the trained MENACE agent\n",
    "menace, _ = training_game.get_players()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print information about how MENACE has performed\n",
    "menace.print_stats()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# play some games against MENACE and see how well he behaves\n",
    "manual_games = 5\n",
    "for _ in range(manual_games):\n",
    "    game = NoughtsAndCrosses(player_one=menace, player_two=HumanPlayer())\n",
    "    game.play(silent=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "\n",
    "### Frage 1: Lass MENACE nun ein einige Male als zweiten Spieler antreten. Was fällt dir auf? Wie lässt sich das erklären?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Frage 2: MENACE wurde bisher nur mittels eines zufällig agierenden Spielers trainiert, sicher gibt es hier noch bessere Möglichkeiten. Was fällt dir hierzu ein? Setze eine Möglichkeit um."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Frage 3: Welche Möglichkeiten fallen dir zur Verbesserung von MENACE ein? Was können wir anders/besser modellieren, damit das Lernen schneller bzw. besser klappt?"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
