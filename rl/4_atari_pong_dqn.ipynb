{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG90pwjB-cIO"
   },
   "source": [
    "# HSKA AI-Lab RL: Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCTFrfsv-g5V"
   },
   "source": [
    "## Mount Google Drive as folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODLMGwrk-gIJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/My\\ Drive/ai-lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3ghLMfm-cIf"
   },
   "source": [
    "Wie in Aufgabe 3 soll nun DQN anhand von [Atari Pong](https://www.gymlibrary.ml/environments/atari/pong/) implementiert werden. Pong zählt zu den klassischen Atari 2600 Spielen und wird hier nahezu unverändert genutzt. Für Details siehe das [Paper von DeepMind](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBDgFNq5-cIf"
   },
   "source": [
    "### Atari Pong Environment vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bD9bTND8-cIp"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install gym[atari]==0.12.5\n",
    "%pip install pyglet==1.3.2\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "import time\n",
    "from datetime import datetime\n",
    "from contextlib import suppress\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Lambda, multiply, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.compat.v1.losses import huber_loss\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from loggers import TensorBoardLogger, tf_summary_image\n",
    "\n",
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plot_utils import plot_statistics\n",
    "from abstract_agent import AbstractAgent\n",
    "from atari_helpers import LazyFrames, wrap_deepmind, make_atari\n",
    "\n",
    "!apt-get install -y xvfb python-opengl\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    from IPython.display import SVG\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D--k_Yo4-cIw"
   },
   "outputs": [],
   "source": [
    "env = make_atari('PongNoFrameskip-v0')\n",
    "env = wrap_deepmind(env, frame_stack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA_EQH3X-cIx"
   },
   "source": [
    "### Aufgabe 4: DQN mit Atari Pong\n",
    "\n",
    "Auch hier soll wieder der `DQNAgent` mit den Methoden `act`, `train` und `_replay` implementiert werden. Das entsprechende Model (siehe `_build_model`) ist dabei wie zuvor bereits vorgegeben. Im Unterschied zur Aufgabe 3 gibt es hier drei zusätzliche Layer bei denen Faltungen vorgenommen werden, um Pixel-basierte Stace-Spaces verarbeiten zu können. Diese Layer sitzen vor den vollverknüpften Layern, um dann anhand der in den Filter detektierten Features globale Entscheidungen treffen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1642061772486,
     "user": {
      "displayName": "Sebastian Blank",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWp2qMl60jFqo-PSVyzCMbWF3twlLxaxhlKrjDeQ=s64",
      "userId": "00888235544304629737"
     },
     "user_tz": -60
    },
    "id": "R6zJhyxa-cI3"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(AbstractAgent):\n",
    "\n",
    "    def __init__(self, action_size: int, state_size: int,\n",
    "                 gamma: float, epsilon: float, epsilon_decay: float, epsilon_min: float, \n",
    "                 alpha: float, batch_size: int, memory_size: int, start_replay_step: int, \n",
    "                 target_model_update_interval: int, train_freq: int):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.step = 0\n",
    "        self.start_replay_step = start_replay_step\n",
    "\n",
    "        self.target_model_update_interval = target_model_update_interval\n",
    "        self.train_freq = train_freq # Frequency (interval) at which model should be trained (steps)\n",
    "\n",
    "        assert self.start_replay_step >= self.batch_size, \\\n",
    "            \"The number of steps to start replay must be at least as large as the batch size\"\n",
    "\n",
    "        self.action_mask = np.ones((1, self.action_size))\n",
    "        self.action_mask_batch = np.ones((self.batch_size, self.action_size))\n",
    "\n",
    "        config = tf.compat.v1.ConfigProto(\n",
    "            intra_op_parallelism_threads=8,\n",
    "            inter_op_parallelism_threads=4,\n",
    "            allow_soft_placement=True,\n",
    "        )\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.compat.v1.Session(config=config)\n",
    "        set_session(session)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Deep Q-network as defined in the DeepMind article on Nature\n",
    "        \n",
    "        Returns:\n",
    "            model [Model]\n",
    "        \"\"\"\n",
    "        atari_shape = (84, 84, 4)\n",
    "        # With the functional API we need to define the inputs.\n",
    "        # Sequential API no longer works because of merge mask\n",
    "        frames_input = Input(atari_shape, name='frames')\n",
    "        action_mask = Input((self.action_size,), name='action_mask')\n",
    "\n",
    "        # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1]\n",
    "        normalized = Lambda(lambda x: x / 255.0, name='normalization')(frames_input)\n",
    "\n",
    "        # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the \n",
    "        # input image and applies a rectifier nonlinearity.\"\n",
    "        conv1 = Conv2D(filters=16,\n",
    "                       kernel_size=(8, 8),\n",
    "                       strides=(4, 4),\n",
    "                       activation='relu')(normalized)\n",
    "\n",
    "        # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed \n",
    "        # by a rectifier nonlinearity.\"\n",
    "        conv2 = Conv2D(filters=32,\n",
    "                       kernel_size=(4, 4),\n",
    "                       strides=(2, 2),\n",
    "                       activation='relu')(conv1)\n",
    "\n",
    "        # Flattening the last convolutional layer.\n",
    "        conv_flattened = Flatten()(conv2)\n",
    "\n",
    "        # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "        hidden = Dense(units=256, activation='relu')(conv_flattened)\n",
    "\n",
    "        # \"The output layer is a fully-connected linear layer with a single output \n",
    "        # for each valid action.\"\n",
    "        output = Dense(self.action_size)(hidden)\n",
    "\n",
    "        filtered_output = multiply([output, action_mask])\n",
    "\n",
    "        model = Model(inputs=[frames_input, action_mask], outputs=filtered_output)\n",
    "        model.compile(loss=huber_loss, optimizer=Adam(lr=self.alpha), metrics=None)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _replay(self) -> None:\n",
    "        \"\"\"Gets random experiences from memory for batch update of Q-function.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Get a random mini-batch from memory and create numpy arrays for each part of this experience\n",
    "        states, actions, next_states, rewards, dones = np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "        \n",
    "\n",
    "        # TODO: Convert the parts of the mini-batch into corresponding numpy arrays.\n",
    "        # Note that the states are of type 'LazyFrames' due to memory efficiency\n",
    "        # and must therefore be converted individually\n",
    "        #states = None\n",
    "        #next_states = None\n",
    "        #actions = None\n",
    "        #rewards = None\n",
    "        #dones = None\n",
    "\n",
    "        # The following assert statements are intended to support further implementation,\n",
    "        # but can also be removed/adjusted if necessary\n",
    "        assert all(isinstance(x, np.ndarray) for x in (states, actions, rewards, next_states, dones)), \\\n",
    "            \"All experience batches should be of type np.ndarray.\"\n",
    "        assert states.shape == (self.batch_size, 84, 84, 4), \\\n",
    "            f\"States shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
    "        assert actions.shape == (self.batch_size,), f\"Actions shape should be: {(self.batch_size,)}\"\n",
    "        assert rewards.shape == (self.batch_size,), f\"Rewards shape should be: {(self.batch_size,)}\"\n",
    "        assert next_states.shape == (self.batch_size, 84, 84, 4), \\\n",
    "            f\"Next states shape should be: {(self.batch_size, 84, 84, 4)}\"\n",
    "        assert dones.shape == (self.batch_size,), f\"Dones shape should be: {(self.batch_size,)}\"\n",
    "\n",
    "        # TODO: Predict the Q values of the next states (choose the right model!). Passing ones as the action mask\n",
    "        # Note that a suitable mask has already been created in '__init__'\n",
    "        next_q_values = None\n",
    "\n",
    "        # TODO: Calculate the Q values, remember\n",
    "        #  - the Q values of each non-terminal state is the reward + gamma * the max next state Q value\n",
    "        #  - and the Q values of terminal states should be the reward (Hint: 1.0 - dones) makes sure that if the game is\n",
    "        #    over, targetQ = rewards\n",
    "        # Depending on the implementation, the axis must be specified to get the max Q value for EACH batch element!\n",
    "        q_values = None\n",
    "\n",
    "        # TODO: Create a one hot encoding of the actions (the selected action is 1 all others 0)\n",
    "        # Hint look at the imports. A Keras help function will be imported there\n",
    "        one_hot_actions = None\n",
    "\n",
    "        # TODO: Create the target Q values based on the one hot encoding of the actions and the calculated Q values\n",
    "        #  Hint you have to \"reshape\" the q_values to match the shape\n",
    "        target_q_values = None\n",
    "\n",
    "        # TODO: Fit the model with the right x and y values\n",
    "        #self.model.fit(\n",
    "        #    x=None,  # states and mask\n",
    "        #    y=None,  # target Q values\n",
    "        #    batch_size=self.batch_size,\n",
    "        #    verbose=0\n",
    "        #)\n",
    "\n",
    "    def act(self, state: LazyFrames) -> int:\n",
    "        \"\"\"Selects the action to be executed based on the given state.\n",
    "\n",
    "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
    "        epsilon, a random action is selected.\n",
    "\n",
    "        Args:\n",
    "            state [LazyFrames]: LazyFrames object representing the state based on 4 stacked observations (images)\n",
    "\n",
    "        Returns:\n",
    "            action [int]\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # TODO: Return random valid action\n",
    "            action = 0\n",
    "        else:\n",
    "            # TODO: Use the model to get the Q values for the state and determine the action based on the max Q value.\n",
    "            # Hint: You have to convert the state to a list of numpy arrays before you can pass it to the model\n",
    "            q_values = None\n",
    "            action = 1\n",
    "        return action\n",
    "\n",
    "    def train(self, experience: Tuple[LazyFrames, int, LazyFrames, float, bool]) -> None:\n",
    "        \"\"\"Stores the experience in memory. If memory is full trains network by replay.\n",
    "\n",
    "        Args:\n",
    "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.memory.append(experience)\n",
    "\n",
    "        # TODO: As soon as enough steps are played:\n",
    "        #  - Update epsilon as long as it is not minimal\n",
    "        #  - Update weights of the target model (syn of the two models)\n",
    "        #  - Execute replay\n",
    "\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1642061772487,
     "user": {
      "displayName": "Sebastian Blank",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWp2qMl60jFqo-PSVyzCMbWF3twlLxaxhlKrjDeQ=s64",
      "userId": "00888235544304629737"
     },
     "user_tz": -60
    },
    "id": "oErKtdA_-cKY"
   },
   "outputs": [],
   "source": [
    "def interact_with_environment(env, agent, n_episodes=600, max_steps=1000000, train=True, verbose=True):      \n",
    "    statistics = []\n",
    "    tb_logger = TensorBoardLogger(f'./logs/run-{datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}')\n",
    "    \n",
    "    with suppress(KeyboardInterrupt):\n",
    "        total_step = 0\n",
    "        for episode in range(n_episodes):\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            state = env.reset()\n",
    "            episode_start_time = time.time()\n",
    "            episode_step = 0\n",
    "\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if train:\n",
    "                    agent.train((state, action, next_state, reward, done))\n",
    "\n",
    "                if episode == 0:\n",
    "                    # for debug purpose log every state of first episode\n",
    "                    for obs in state:\n",
    "                        tb_logger.log_image(f'state_t{episode_step}:', tf_summary_image(np.array(obs, copy=False)),\n",
    "                                            global_step=total_step)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_step += 1\n",
    "            \n",
    "            total_step += episode_step\n",
    "\n",
    "            if episode % 10 == 0:\n",
    "                speed = episode_step / (time.time() - episode_start_time)\n",
    "                tb_logger.log_scalar('score', episode_reward, global_step=total_step)\n",
    "                tb_logger.log_scalar('epsilon', agent.epsilon, global_step=total_step)\n",
    "                tb_logger.log_scalar('speed', speed, global_step=total_step)\n",
    "                if verbose:\n",
    "                    print(f'episode: {episode}/{n_episodes}, score: {episode_reward}, steps: {episode_step}, '\n",
    "                          f'total steps: {total_step}, e: {agent.epsilon:.3f}, speed: {speed:.2f} steps/s')\n",
    "\n",
    "            statistics.append({\n",
    "                'episode': episode,\n",
    "                'score': episode_reward,\n",
    "                'steps': episode_step\n",
    "            })\n",
    "                                  \n",
    "            if total_step >= max_steps:\n",
    "                break\n",
    "        \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7leVkoX-cKe"
   },
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "# Hyperparams (should be sufficient)\n",
    "annealing_steps = 100000  # not episodes!\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = (epsilon - epsilon_min) / annealing_steps\n",
    "alpha = 0.0001\n",
    "batch_size = 64\n",
    "memory_size = 10000\n",
    "start_replay_step = 10000\n",
    "target_model_update_interval = 1000\n",
    "train_freq = 4\n",
    "\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size, gamma=gamma, \n",
    "                 epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min, \n",
    "                 alpha=alpha, batch_size=batch_size, memory_size=memory_size,\n",
    "                 start_replay_step=start_replay_step, \n",
    "                 target_model_update_interval=target_model_update_interval, train_freq=train_freq)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, n_episodes=400, verbose=True)\n",
    "env.close()\n",
    "plot_statistics(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu2gsh0c-cKf"
   },
   "source": [
    "#### Aufbau Keras Model\n",
    "Der Aufbau des Keras-Modells kann zur Verdeutlichung nochmals geplottet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSzb9hox-cKg"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(agent.model, to_file='keras_plot_model.png', show_shapes=True)\n",
    "display.Image('keras_plot_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1wFDzqO-cKg"
   },
   "source": [
    "#### Performanceauswertung (Video)\n",
    "Der folgende Code dient zur Performancebewertung des Agenten. Der (hoffentlich) trainierte Agent wird bei seiner Ausführung gefilmt, trainiert aber nicht weiter. Anschließend wird das Video seiner besten Performance dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn0FJGjL-cKh"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2SqJWz1-cKi"
   },
   "source": [
    "### Quiz: DQN\n",
    "\n",
    "#### Frage 1: Wie könnte man das Lernen weiter optimieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpZN1NV7-cKj"
   },
   "source": [
    "Antwort: # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmnPoX7N-cKj"
   },
   "source": [
    "#### Frage 2: Welche Ansätze hat DeepMind verfolgt, um das Lernen weiter zu optimieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw7yCfzb-cKj"
   },
   "source": [
    "Antwort: # TODO (**Hinweis**: Double DQN, Dueling und Prioritzed Replay)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_atari_pong_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
