{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im6Lg8rI28or"
   },
   "source": [
    "# HSKA AI-Lab RL: Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTggVhg03Sfq"
   },
   "source": [
    "## Mount Google Drive as folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZYyfG193KJV"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/My\\ Drive/ai-lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hJ7s-MI3b9X"
   },
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8blNLTFM28ow"
   },
   "source": [
    "In dem vorherigen Notebook wurde für die Verwendung mit dem Q-Learning Algorithmus der State-Space diskretisiert, um die Komplexität zu reduzieren. In dieser Aufgabe soll nun der State-Space unverändert in der Q-Funktion abgebildet werden, ohne Informationen zu verwerfen. Dafür soll Q-Learning so erweitert werden, dass die Q-Funktion mithilfe eines neuronalen Netzes trainiert werden kann. Wie in Aufgabe 2, soll dies zunächst anhand des Beispiels [CartPole](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) implementiert werden. Zur Wiederholung: CartPole steht im OpenAI Gym zur Verfügung und wird in der Implementierung (`CartPoleEnv`) wie folgt beschrieben:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "a frictionless track. The pendulum starts upright, and the goal is to\n",
    "prevent it from falling over by increasing and reducing the cart's\n",
    "velocity.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvW_uo-C28ox"
   },
   "source": [
    "### CartPole Environment vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PI9ueML028oy"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install tensorflow\n",
    "%pip install gym[atari]==0.12.5\n",
    "%pip install pyglet==1.3.2\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "from typing import Tuple\n",
    "import time\n",
    "from contextlib import suppress\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, multiply\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plot_utils import plot_statistics\n",
    "import check_test\n",
    "from abstract_agent import AbstractAgent\n",
    "\n",
    "!apt-get install -y xvfb python-opengl\n",
    "%pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knwpPs4728o5"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9jdpYyR28o6"
   },
   "source": [
    "### Aufgabe 3.1: Deep Q-Learning mit CartPole\n",
    "\n",
    "Im ersten Schritt zu einem vollwertigen Deep Reinforcement Learning Agent soll die bisher in Form einer Tabelle abgebildete Q-Funktion durch ein neuronales Netz ersetzt werden. Für die Implementierung des Netzes wird Keras mit einem TensorFlow Backend verwendet. Das entsprechende Model (siehe `build_model`) ist dabei bereits vorgegeben und muss nicht selbst implementiert werden. Dieses besteht aus zwei vollverknüpften Hidden Layer und einem entsprechenden Ein- und Ausgangs-Layer. Wie im Rahmen der theoretischen Einführung gezeigt, werden aus Performance-Gründen Action-Values für jede Action nur anhand des States berechnet. Das bedeutet, dass der Input des Netzes entsprechend aus den Feature (= State) besteht und der Ausgang aus den möglichen Actions. Die in dem jeweiligen State gewählte Action wird im Ausgang maskiert, sodass nur diese \"aktiviert\" (mit 1.0 multipliziert) wird und die anderen Actions entsprechend auf 0.0 gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1642060146060,
     "user": {
      "displayName": "Sebastian Blank",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWp2qMl60jFqo-PSVyzCMbWF3twlLxaxhlKrjDeQ=s64",
      "userId": "00888235544304629737"
     },
     "user_tz": -60
    },
    "id": "16fqx87r28o7"
   },
   "outputs": [],
   "source": [
    "def build_model(state_size: int, action_size: int, alpha: float):\n",
    "    # With the functional API we need to define the inputs.\n",
    "    # Sequential API no longer works because of merge mask\n",
    "    states_input = Input((state_size,), name='states')\n",
    "    action_mask = Input((action_size,), name='action_mask')\n",
    "\n",
    "    hidden_1 = Dense(units=32, activation='relu')(states_input)\n",
    "    hidden_2 = Dense(units=32, activation='relu')(hidden_1)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = Dense(action_size, activation='linear')(hidden_2)\n",
    "\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    # The main drawback of [passing the action as an input] is that a separate forward pass is required\n",
    "    # to compute the Q-value of each action, resulting in a cost that scales linearly with the number of\n",
    "    # actions. We instead use an architecture in which there is a separate output unit for each possible\n",
    "    # action, and only the state representation is an input to the neural network.\n",
    "    # The outputs correspond to the predicted Q-values of the individual action for the input state.\n",
    "    # The main advantage of this type of architecture is the ability to compute Q-values for\n",
    "    # all possible actions in a given state with only a single forward pass through the network.\n",
    "    filtered_output = multiply([output, action_mask])\n",
    "\n",
    "    model = Model(inputs=[states_input, action_mask], outputs=filtered_output)\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=alpha), metrics=None)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D8t349u28o9"
   },
   "source": [
    "#### Aufbau Keras Model\n",
    "\n",
    "Nachfolgend wird zur Verdeutlichung des Keras Modells dieses nochmal geplottet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKLlM5Nf28o9"
   },
   "outputs": [],
   "source": [
    "model = build_model(state_size=env.observation_space.shape[0],\n",
    "                    action_size=env.action_space.n,\n",
    "                    alpha=0.001)\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='keras_plot_model.png', show_shapes=True)\n",
    "display.Image('keras_plot_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SLc8rIw28pD"
   },
   "source": [
    "#### Deep Q-Learning Agent\n",
    "\n",
    "Für den `AdvancedQAgent`, welcher wieder von `AbstractAgent` erbt, muss zu den bisherigen Methoden `act` und `train` (`__init__` ist hier bereits vorgegeben) noch die private Methode `_replay` implementiert werden. Der Agent besitzt eine Art Gedächtnis (siehe `memory` in der `__init__` Methode), welches als Ring-Buffer implementiert ist. Dieses Gedächtnis dient dazu, beobachtete States zu persistieren, um diese Erfahrungen dann als Mini-Batch beim Lernen an den Optimizer des neuronalen Netzes zu schicken. Die Größe des Gedächtnisses entspricht der eines Mini-Batches. In der `_replay` Methode werden die States aus dem Gedächtnis ausgelesen, um die Q-Funktion zu updaten.\n",
    "\n",
    "Der `AdvancedQAgent` entspricht dabei noch nicht der Implementierung des in der theoretischen Einführung vorgestelltem Deep Q-Network. In diesem ersten naiven Ansatz, wird das Gedächtnis noch nicht im Sinne des im [Paper von DeepMind](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) gezeigten Replay Memory eingesetzt, sondern lediglich als Speicher für das Sammeln von Erfahrungen für das Batch-Update. Auch gibt es in diesem ersten Ansatz noch kein Target Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1642060148449,
     "user": {
      "displayName": "Sebastian Blank",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWp2qMl60jFqo-PSVyzCMbWF3twlLxaxhlKrjDeQ=s64",
      "userId": "00888235544304629737"
     },
     "user_tz": -60
    },
    "id": "ka7dnlJm28pE"
   },
   "outputs": [],
   "source": [
    "class AdvancedQAgent(AbstractAgent):\n",
    "\n",
    "    def __init__(self, action_size: int, state_size: int, model: Model, batch_size: int, \n",
    "                 gamma: float, epsilon: float, epsilon_decay: float, epsilon_min: float):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.gamma = gamma # discount factor (how much discount future reward)\n",
    "        self.epsilon = epsilon # initial exploration rate of the agent (exploitation vs. exploration)\n",
    "        self.epsilon_min = epsilon_min # minimal epsilon: x% of the time take random action\n",
    "        self.epsilon_decay = epsilon_decay # decay epsilon over time to shift from exploration to exploitation\n",
    "        \n",
    "        self.step = 0 # number of steps played\n",
    "        \n",
    "        # Remembered states and rewards which are used for learning in mini-batch volumes\n",
    "        self.memory = deque(maxlen=batch_size)\n",
    "        \n",
    "        self.model = model\n",
    "        self.batch_size = batch_size # can be any multiple of 32 (smaller mini-batch size usually leads to higher accuracy)\n",
    "\n",
    "        self.action_mask = np.ones((1, self.action_size))\n",
    "        self.action_mask_batch = np.ones((self.batch_size, self.action_size))\n",
    "\n",
    "    def _replay(self) -> None:\n",
    "        \"\"\"Gets collected experiences from memory for batch update of Q-function.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Get mini-batch from memory and create numpy arrays for each part of this experience\n",
    "        states, actions, next_states, rewards, dones = np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "        \n",
    "        # The following assert statements are intended to support further implementation,\n",
    "        # but can also be removed/adjusted if necessary\n",
    "        assert all(isinstance(x, np.ndarray) for x in (states, actions, rewards, next_states, dones)), \\\n",
    "            \"All experience batches should be of type np.ndarray.\"\n",
    "        assert states.shape == (self.batch_size, self.state_size), \\\n",
    "            f\"States shape should be: {(self.batch_size, self.state_size)}\"\n",
    "        assert actions.shape == (self.batch_size,), f\"Actions shape should be: {(self.batch_size,)}\"\n",
    "        assert rewards.shape == (self.batch_size,), f\"Rewards shape should be: {(self.batch_size,)}\"\n",
    "        assert next_states.shape == (self.batch_size, self.state_size), \\\n",
    "            f\"Next states shape should be: {(self.batch_size, self.state_size)}\"\n",
    "        assert dones.shape == (self.batch_size,), f\"Dones shape should be: {(self.batch_size,)}\"\n",
    "\n",
    "        # TODO: Predict the Q values of the next states. Passing ones as the action mask\n",
    "        next_q_values = None\n",
    "\n",
    "        # TODO: Set the Q values of terminal states to 0 (by definition)\n",
    "\n",
    "        # TODO: Calculate the Q values, remember the Q values of each non-terminal state is the reward + gamma * the max next state Q value.\n",
    "        # Important: Depending on the implementation, the axis must be specified to get the max Q value for EACH batch element!\n",
    "        q_values = None\n",
    "\n",
    "\n",
    "        # TODO: Create a one hot encoding of the actions (the selected action is 1 all others 0)\n",
    "        one_hot_actions = None\n",
    "\n",
    "        # TODO: Create the target Q values based on the one hot encoding of the actions and the calculated Q values\n",
    "        target_q_values = None\n",
    "\n",
    "        # TODO: Fit the model with the right x and y values\n",
    "        #self.model.fit(\n",
    "        #    x=None,  # states and mask\n",
    "        #    y=None,  # target Q values\n",
    "        #    batch_size=self.batch_size,\n",
    "        #    verbose=0\n",
    "        #)\n",
    "\n",
    "    def act(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Selects the action to be executed based on the given state.\n",
    "\n",
    "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
    "        epsilon, a random action is selected.\n",
    "\n",
    "        Args:\n",
    "            state [array]: Numpy array with shape (1,4) representing the state based on 4 float values:\n",
    "                - cart position,\n",
    "                - cart velocity,\n",
    "                - angle,\n",
    "                - angle velocity\n",
    "\n",
    "        Returns:\n",
    "            action [int]\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # TODO: Return random valid action\n",
    "            action = 0\n",
    "        else:\n",
    "            # TODO: Use the model to get the Q values for the state and determine the action based on the max Q value\n",
    "            action = 1\n",
    "        return action\n",
    "\n",
    "    def train(self, experience: Tuple[np.ndarray, int, np.ndarray, float, bool]) -> None:\n",
    "        \"\"\"Stores the experience in memory. If memory is full trains network by replay.\n",
    "\n",
    "        Args:\n",
    "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Persist experience for training with mini-batches\n",
    "        self.memory.append(experience)\n",
    "\n",
    "        # TODO: As soon as enough steps are played:\n",
    "        #  - Update epsilon as long as it is above threshold (minimal epsilon)\n",
    "        #  - Execute replay\n",
    "\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1642060148454,
     "user": {
      "displayName": "Sebastian Blank",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWp2qMl60jFqo-PSVyzCMbWF3twlLxaxhlKrjDeQ=s64",
      "userId": "00888235544304629737"
     },
     "user_tz": -60
    },
    "id": "JyR8QtIx28pF"
   },
   "outputs": [],
   "source": [
    "def interact_with_environment(env, agent, n_episodes=500, max_steps=200, train=True, verbose=True):      \n",
    "    statistics = []\n",
    "    \n",
    "    with suppress(KeyboardInterrupt):\n",
    "        for episode in range(n_episodes):\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            state = env.reset()\n",
    "            episode_start_time = time.time()\n",
    "\n",
    "            for t in range(max_steps):\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if train:\n",
    "                    agent.train((state, action, next_state, reward, done))\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if verbose and episode % 10 == 0:\n",
    "                speed = t / (time.time() - episode_start_time)\n",
    "                print(f'episode: {episode}/{n_episodes}, score: {total_reward}, steps: {t}, '\n",
    "                      f'e: {agent.epsilon:.3f}, speed: {speed:.2f} steps/s')\n",
    "\n",
    "            statistics.append({\n",
    "                'episode': episode,\n",
    "                'score': total_reward,\n",
    "                'steps': t\n",
    "            })\n",
    "        \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHbvrw8U28pF"
   },
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "# Hyperparams (should be sufficient)\n",
    "gamma = 1.0\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.001\n",
    "epsilon_decay = 0.0005\n",
    "alpha = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "model = build_model(state_size=state_size, action_size=action_size, alpha=alpha)\n",
    "\n",
    "agent = AdvancedQAgent(action_size=action_size, state_size=state_size, model=model, batch_size=batch_size, \n",
    "                       gamma=gamma, epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, n_episodes=500, verbose=True)\n",
    "env.close()\n",
    "plot_statistics(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3yMNBmP28pG"
   },
   "source": [
    "Der `AdvancedQAgent` sollte nun trainierbar sein und im Plot der Scores (über etwa 500 Episoden) sollte man einen Aufwärtstrend ab etwa Episode 200 erkennen. Trotz gut gewählter Hyperparameter wird der Agent jedoch ziemlich instabil sein, mit Scores die zwar immer wieder die 200 erreichen, zwischendurch jedoch häufig bis auf einstellige Werte zurückfallen. Der erreichte Score beim CartPole wird auch nach 500 Episoden noch stark schwanken. Diese Instabilität gilt es im nächsten Schritt zu beheben.\n",
    "\n",
    "Mit dem `AdvancedQAgent` wurde zwar praktisch ein Deep Q-Learning Agent eingeführt, dieser ist jedoch leider nicht zu gebrauchen. Vor diesem Problem standen auch die Leute von DeepMind 2013 bzw. 2015, infolgedessen DQN erschaffen wurde. Deep Q-Network (DQN) machte als erster Algorithmus die Verwendung von neuronalen Netzen im Reinforcement Learning Bereich nutzbar. Die oben erfahrenen Instabilitäten wurden dabei mittels Replay Memory und Target Network angegangen. An dieser Stelle soll nun entsprechend der `AdvancedQAgent` um genau diese Fähigkeiten erweitert werden.\n",
    "\n",
    "In Aufgabe 3.2 wird das DQN CartPole angewendet, dies ist hauptsächlich als zusätzlicher Verständnisschritt gedacht. In Aufgabe 4 wird ebenfalls das DQN angewendet, allerdings dann auf Atari Pong. Daher kann Aufgabe 3.2 auch überprungen werden und ist als optional anzusehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDC_QyaT28pG"
   },
   "source": [
    "### Aufgabe 3.2: DQN mit CartPole (optional)\n",
    "Für den `DQNAgent` wird nun in der `__init__` Methode ein zusätzliches Target Network angelegt, welches es in regelmäßigen Abständen mit den Gewichten aus dem anderen Netzwerk zu aktualisieren gilt. Der Replay Memory (`memory`) soll nun nicht mehr nur als Mini-Batch Zwischenspeicher dienen, sondern eine größere Ansammlung an States enthalten, über welche zufällig gesampled werden kann, um Korrelationen von sequentiellen States aufzubrechen und damit das Training zu stabilisieren.\n",
    "\n",
    "An dem Model wird nichts verändert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1642060285946,
     "user": {
      "displayName": "Sebastian Blank",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhWp2qMl60jFqo-PSVyzCMbWF3twlLxaxhlKrjDeQ=s64",
      "userId": "00888235544304629737"
     },
     "user_tz": -60
    },
    "id": "PhZpLV_r28pH"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(AbstractAgent):\n",
    "\n",
    "    def __init__(self, action_size: int, state_size: int, model: Model, target_model: Model, batch_size: int, \n",
    "                 gamma: float, epsilon: float, epsilon_decay: float, epsilon_min: float, \n",
    "                 start_replay_step: int, target_model_update_interval: int):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.gamma = gamma # discount factor (how much discount future reward)\n",
    "        self.epsilon = epsilon # initial exploration rate of the agent (exploitation vs. exploration)\n",
    "        self.epsilon_min = epsilon_min # minimal epsilon: x% of the time take random action\n",
    "        self.epsilon_decay = epsilon_decay # decay epsilon over time to shift from exploration to exploitation\n",
    "        \n",
    "        # Remembered states and rewards which are used for learning in mini-batch volumes\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        self.model = model\n",
    "        self.target_model = target_model\n",
    "        self.batch_size = batch_size # can be any multiple of 32 (smaller mini-batch size usually leads to higher accuracy)\n",
    "        \n",
    "        self.step = 0 # number of steps played\n",
    "        \n",
    "        self.start_replay_step = start_replay_step # After how many played steps the experience replay should start\n",
    "        self.target_model_update_interval = target_model_update_interval # After how many steps should the weights of the target model be updated\n",
    "\n",
    "        assert self.start_replay_step >= self.batch_size, \\\n",
    "            \"The number of steps to start replay must be at least as large as the batch size\"\n",
    "\n",
    "        self.action_mask = np.ones((1, self.action_size))\n",
    "        self.action_mask_batch = np.ones((self.batch_size, self.action_size))\n",
    "\n",
    "    def _replay(self) -> None:\n",
    "        \"\"\"Sample random over collected experiences from memory for batch update of Q-function.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Get a random mini-batch from memory and create numpy arrays for each part of this experience\n",
    "        states, actions, next_states, rewards, dones = np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "        # The following assert statements are intended to support further implementation,\n",
    "        # but can also be removed/adjusted if necessary.\n",
    "        assert all(isinstance(x, np.ndarray) for x in (states, actions, rewards, next_states, dones)), \\\n",
    "            \"All experience batches should be of type np.ndarray.\"\n",
    "        assert states.shape == (self.batch_size, self.state_size), \\\n",
    "            f\"States shape should be: {(self.batch_size, self.state_size)}\"\n",
    "        assert actions.shape == (self.batch_size,), f\"Actions shape should be: {(self.batch_size,)}\"\n",
    "        assert rewards.shape == (self.batch_size,), f\"Rewards shape should be: {(self.batch_size,)}\"\n",
    "        assert next_states.shape == (self.batch_size, self.state_size), \\\n",
    "            f\"Next states shape should be: {(self.batch_size, self.state_size)}\"\n",
    "        assert dones.shape == (self.batch_size,), f\"Dones shape should be: {(self.batch_size,)}\"\n",
    "\n",
    "        # TODO: Predict the Q values of the next states. Passing ones as the action mask\n",
    "        next_q_values = None\n",
    "\n",
    "        # TODO: Set the Q values of terminal states to 0 (by definition)\n",
    "\n",
    "        # TODO: Calculate the Q values, remember the Q values of each non-terminal state is the reward + gamma * the max next state Q value\n",
    "        # Depending on the implementation, the axis must be specified to get the max Q value for EACH batch element!\n",
    "        q_values = None\n",
    "\n",
    "        # TODO: Create a one hot encoding of the actions (the selected action is 1 all others 0)\n",
    "        one_hot_actions = None\n",
    "\n",
    "        # TODO: Create the target Q values based on the one hot encoding of the actions and the calculated Q values\n",
    "        target_q_values = None\n",
    "\n",
    "        # TODO: Fit the model with the right x and y values\n",
    "        #self.model.fit(\n",
    "        #    x=None,  # states and mask\n",
    "        #    y=None,  # target Q values\n",
    "        #    batch_size=self.batch_size,\n",
    "        #    verbose=0\n",
    "        #)\n",
    "\n",
    "    def act(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Selects the action to be executed based on the given state.\n",
    "\n",
    "        Implements epsilon greedy exploration strategy, i.e. with a probability of\n",
    "        epsilon, a random action is selected.\n",
    "\n",
    "        Args:\n",
    "            state [array]: Numpy array with shape (1,4) representing the state based on 4 float values:\n",
    "                - cart position,\n",
    "                - cart velocity,\n",
    "                - angle,\n",
    "                - angle velocity\n",
    "\n",
    "        Returns:\n",
    "            action [int]\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # TODO: Return random valid action\n",
    "            action = 0\n",
    "        else:\n",
    "            # TODO: Use the model to get the Q values for the state and determine the action based on the max Q value\n",
    "            action = 1\n",
    "        return action\n",
    "\n",
    "    def train(self, experience: Tuple[np.ndarray, int, np.ndarray, float, bool]) -> None:\n",
    "        \"\"\"Stores the experience in memory. If memory is full trains network by replay.\n",
    "\n",
    "        Args:\n",
    "            experience [tuple]: Tuple of state, action, next state, reward, done.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.memory.append(experience)\n",
    "\n",
    "        # TODO: As soon as enough steps are played:\n",
    "        #  - Update epsilon as long as it above threshold\n",
    "        #  - Update weights of the target model (syn of the two models)\n",
    "        #  - Execute replay\n",
    "\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMPzS3vA28pI"
   },
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "# Hyperparams (should be sufficient)\n",
    "gamma = 1.0\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.001\n",
    "epsilon_decay = 0.0005\n",
    "alpha = 0.001\n",
    "batch_size = 32\n",
    "memory_size = 50000\n",
    "start_replay_step = 2000\n",
    "target_model_update_interval = 1000\n",
    "\n",
    "model = build_model(state_size=state_size, action_size=action_size, alpha=alpha)\n",
    "target_model = build_model(state_size=state_size, action_size=action_size, alpha=alpha)\n",
    "\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size, model=model, target_model=target_model, batch_size=batch_size, \n",
    "                 gamma=gamma, epsilon=epsilon, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min, \n",
    "                 start_replay_step=start_replay_step, target_model_update_interval=target_model_update_interval)\n",
    "\n",
    "statistics = interact_with_environment(env, agent, n_episodes=500, verbose=True)\n",
    "env.close()\n",
    "plot_statistics(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vw5qIVva28pI"
   },
   "outputs": [],
   "source": [
    "assert len(statistics) >= 100, 'Please run at least 100 episodes for validating result'\n",
    "print('Mean over last 100 episodes: {}'.format(np.mean(list(map(itemgetter('score'), statistics[-100:])))))\n",
    "check_test.run_check('mean_score_check', statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yHKeEFh28pJ"
   },
   "source": [
    "#### Performanceauswertung (Video)\n",
    "\n",
    "Der folgende Code dient zur Performancebewertung des Agenten. Der (hoffentlich) trainierte Agent wird bei seiner Ausführung gefilmt, trainiert aber nicht weiter. Anschließend wird das Video seiner besten Performance dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJRugJJt28pJ"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_cart_pole_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
